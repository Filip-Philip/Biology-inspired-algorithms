{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoznawanie owoców i warzyw\n",
    "\n",
    "Jako temat naszego projektu wybraliśmy rozpoznawanie różnych gatunków owoców i warzyw ze zdjęcia. \n",
    "\n",
    "## Dane techniczne\n",
    "\n",
    "Komputer z system **Windows 11**\n",
    "\n",
    "Procesor: **Intel(R) Core i5-6400 CPU Nvidia GForce GTX 1060** \n",
    "\n",
    "Język: **Python** \n",
    "\n",
    "Środowisko: **Jupyter notebook** \n",
    "\n",
    "Biblioteki: **PyTorch**\n",
    "\n",
    "## Baza obrazów\n",
    "Baza obrazów została pobrana ze strony [kaggle](https://www.kaggle.com/moltean/fruits). Obrazy w niej zawarte przedstawiają kilkadziesiąt różnych gatunków warzyw i owoców. Obrazy mają rozmiary 100 na 100 pikseli, i są robione na białym tle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budowanie modelu\n",
    "\n",
    "Sieć składa się z dwóch warstw konwolucyjnych, które bardzo ułatwiają analize obrazów, a nastepnie dwóch wartw \"zwykłych\", gdzie każdy neuron jest połączony z każdym nastepnym neuronem. Jako funckję aktywacji korzystaliśmy z wbudowanej funkcji ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WlMr0vpZfkKC"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2cVVYNvlf6fU"
   },
   "outputs": [],
   "source": [
    "class LeNet(Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        krnl_s = 5\n",
    "        ch = 20\n",
    "        \n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=ch,\n",
    "            kernel_size=(krnl_s, krnl_s))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=ch, out_channels=50,\n",
    "            kernel_size=(krnl_s, krnl_s))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        self.fc1 = Linear(in_features=105800, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        self.fc2 = Linear(in_features=500, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        \n",
    "        return output\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AJB_hvVjgfyd"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustawienia\n",
    "Obrazki były dzielone na grupy po 64. Sieć była uczona przez 2 epochy, ponieważ później następowało przetrenowanie. 75% danych było wykorzystywane do uczenia a 25% do walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sF84_mpxhPeH"
   },
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ładowanie danych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vqNhKYXmiHKe"
   },
   "outputs": [],
   "source": [
    "trainData  = ImageFolder('proj1/archive/fruits-360_dataset/fruits-360/Training', transform=ToTensor())\n",
    "\n",
    "numTrainSamples = int(len(trainData) * TRAIN_SPLIT)\n",
    "numValSamples = int(len(trainData) * VAL_SPLIT)\n",
    "(trainData, valData) = random_split(trainData,\n",
    "\t[numTrainSamples, numValSamples],\n",
    "\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V1tjdOEJiN98"
   },
   "outputs": [],
   "source": [
    "testData  = ImageFolder('proj1/archive/fruits-360_dataset/fruits-360/Test', transform=ToTensor())\n",
    "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
    "\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ddfb4kWdiY41"
   },
   "outputs": [],
   "source": [
    "model = LeNet(numChannels=3, classes=len(trainData.dataset.classes)).to(device)\n",
    "\n",
    "\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()\n",
    "\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCOthObIieBX"
   },
   "outputs": [],
   "source": [
    "\n",
    "for e in range(0, EPOCHS):\n",
    "\tmodel.train()\n",
    "    \n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "    \n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "    \n",
    "\tfor (x, y) in trainDataLoader:\n",
    "        \n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "        \n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFn(pred, y)\n",
    "        \n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "        \n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "        \n",
    "        \n",
    "\twith torch.no_grad():\n",
    "        \n",
    "\t\tmodel.eval()\n",
    "        \n",
    "\t\tfor (x, y) in valDataLoader:\n",
    "            \n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "            \n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotalValLoss += lossFn(pred, y)\n",
    "            \n",
    "\t\t\tvalCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n",
    "            \n",
    "            \n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "    \n",
    "\ttrainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
    "\tvalCorrect = valCorrect / len(valDataLoader.dataset)\n",
    "    \n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"train_acc\"].append(trainCorrect)\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\tH[\"val_acc\"].append(valCorrect)\n",
    "    \n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, trainCorrect))\n",
    "\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "\t\tavgValLoss, valCorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykres poprawności modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "52boCiS8imUS"
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.savefig(f\"plot_save_{EPOCHS}epoch.pdf\")\n",
    "torch.save(model, f\"model_{EPOCHS}epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy wykres prezentuje dokładność sieci neuronowej dla danych testowych i dla danych walidacyjnych (przy których wagi w sieci się nie zmieniały). Oś pozioma wskazuje epochy (od 0), a pionowa dokładność w skali od 0 do 1.\n",
    "\n",
    "![wykres](plot_save_2epoch-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie pojedynczych obrazków:\n",
    "W bazie danych był też dostępny zbiór dodatkowych obrazów do testowania. Najpierw sprawdziliśmy jak sieć radzi sobie z rozpoznawaniem pojedynczych obrazków.\n",
    "\n",
    "![cherry](test_dataset_cherry.jpg)\n",
    "![cherry](test_dataset_banana.jpg)\n",
    "![cherry](test_dataset_apple.jpg)\n",
    "![cherry](test_dataset_avocado.jpg)\n",
    "![cherry](test_dataset_peach.jpg)\n",
    "\n",
    "Dodatkowo zrobiliśmy zdjęcie papryki, poddaliśmy lekkiej obróbce (zmniejszyliśmy rozmiar, ustawiliśmy tło na białe, i lekko zmieniliśmy kolorystykę ze względu na inne warunki oświetleniowe) i ją też przetestowaliśmy.\n",
    "\n",
    "![cherry](papryka100x100_postprocess.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def label_single_image(model_path, image_path, my_model = None):\n",
    "\tif my_model == None:\n",
    "\t\tmy_model = torch.load(model_path, map_location=torch.device('cpu')).to(device)\n",
    "\twith torch.no_grad():\n",
    "        \n",
    "\t\tmy_model.eval()\n",
    "\n",
    "\t\timg = Image.open(image_path)\n",
    "\t\t\n",
    "\t\tconvert_tensor = transforms.ToTensor()\n",
    "\t\timg_tensor = convert_tensor(img)\n",
    "\t\timg_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "\t\timg_tensor = img_tensor.to(device)\n",
    "\t\tpred = my_model(img_tensor)\n",
    "\n",
    "\t\tlabel_id = pred.max(1).indices\n",
    "\t\tfor l_id in label_id:\n",
    "\t\t\treturn testData.classes[l_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cherry 1\n",
      "Pepper Red\n",
      "Banana\n",
      "Apple Braeburn\n",
      "Avocado\n",
      "Peach\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(label_single_image(\"model_2epoch\", \"test_dataset_cherry.jpg\"))\n",
    "print(label_single_image(\"model_2epoch\", \"papryka100x100_postprocess.jpg\"))\n",
    "print(label_single_image(\"model_2epoch\", \"test_dataset_banana.jpg\"))\n",
    "print(label_single_image(\"model_2epoch\", \"test_dataset_apple.jpg\"))\n",
    "print(label_single_image(\"model_2epoch\", \"test_dataset_avocado.jpg\"))\n",
    "print(label_single_image(\"model_2epoch\", \"test_dataset_peach.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać dla tych obrazków sieć wskazywała poprawne etykiety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie dużej ilości danych\n",
    "Przetestowaliśmy też większą ilość obrazków ze zbioru testowego. Jak widać poniżej otrzymaliśmy dokładność 97.84871616932685%. Dodatkowo warto zauważyć że duża ilość błędów (poniżej kodu wypisane są niedopasowania) wynikają z pogrupowania danych w bazie np. istnieją dwie etykiety \"Apple Red 1\" i \"Apple Red 2\", które sieć traktuje jako dwa osobne owoce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Apple Red 1\\321_100.jpg | Apple Red 2 != Apple Red 1\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Apple Red 1\\322_100.jpg | Apple Red 2 != Apple Red 1\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Apple Red 1\\323_100.jpg | Apple Red 2 != Apple Red 1\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Cantaloupe 2\\161_100.jpg | Apple Golden 1 != Cantaloupe 2\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Cantaloupe 2\\163_100.jpg | Apple Golden 1 != Cantaloupe 2\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\100_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\101_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\102_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\104_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\105_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\106_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\107_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\108_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\109_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\110_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Eggplant\\111_100.jpg | Plum 3 != Eggplant\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Nectarine\\35_100.jpg | Tamarillo != Nectarine\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\0_100.jpg | Pear Kaiser != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\103_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\104_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\105_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\106_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\107_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\108_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Strawberry Wedge\\109_100.jpg | Onion Red Peeled != Strawberry Wedge\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\10_100.jpg | Nectarine Flat != Tomato 3\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\11_100.jpg | Nectarine Flat != Tomato 3\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\12_100.jpg | Nectarine Flat != Tomato 3\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\13_100.jpg | Nectarine Flat != Tomato 3\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\14_100.jpg | Nectarine Flat != Tomato 3\n",
      "proj1/archive/fruits-360_dataset/fruits-360/Test\\Tomato 3\\15_100.jpg | Nectarine Flat != Tomato 3\n",
      "97.84871616932685%"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def test_all_files(dir):\n",
    "    my_model = torch.load(\"model_2epoch\", map_location=torch.device('cpu')).to(device)\n",
    "\n",
    "    all_count = 0\n",
    "    correct_count = 0\n",
    "    for file in os.scandir(dir):\n",
    "        if file.is_dir():\n",
    "            label = file.name\n",
    "            img_count = 0\n",
    "            for img in os.scandir(file):\n",
    "                all_count += 1\n",
    "                res = label_single_image(\"\", img.path, my_model)\n",
    "                if label != res:\n",
    "                    print(img.path, \"|\", res, \"!=\", label)\n",
    "                else:\n",
    "                    correct_count +=1\n",
    "\n",
    "                if img_count < 10:\n",
    "                    img_count += 1\n",
    "                else:\n",
    "                    break\n",
    "    return correct_count / all_count\n",
    "\n",
    "print(test_all_files(\"proj1/archive/fruits-360_dataset/fruits-360/Test\") * 100, end=\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "Otrzymana sieć bardzo dobrze radziła sobie na obrazkach z bazy danych, jednak były one poddane dużej obróbce także sieć prawdopodobnie nie radziła by sobie dla zdjęć zrobionych w innych warunkach."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "owocki.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
